\documentclass[10pt]{beamer}

\mode<presentation>
{
  \usetheme{Darmstadt}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{rose} % or try albatross, beaver, crane, ...
  \usefonttheme{structurebold}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{mathtools}
\usepackage{bbm}

% \usepackage{amsfonts}
% \usepackage{amsmath}
% \usepackage{amssymb}

\title[Life Insurance Mathematics]{Exam Presentation \\Life Insurance Mathematics}
\author{Sami Zweidler}
\institute{ETH Zurich}
\date{20/01/2026}

\AtBeginSection[]{
  \begin{frame}[plain]
    \centering
    \vfill
    {\Huge\insertsection}
    \vfill
  \end{frame}
}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}[plain]
	\tableofcontents
\end{frame}
% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

% \section{Task 1: Markov Model}
\section{Task 1: Markov Model}
\begin{frame}{Markov Chains}
	What is a Markov Chain?\\
	It is a Stochastic model that describes sequence of transitions from one state to another according to certain probabilistic rules.
	\begin{figure}
		\includegraphics[width=0.5\textwidth]{Example_Markov.png}
	\end{figure}
\end{frame}


\begin{frame}{Markow Chains}
    \begin{block}{Definition}
		$(X_t)_{t\in \mathbb{N}}: (\Omega, \mathcal{A}, \mathbb{P}) \to \mathcal{S} = \{1,2,3, \ldots\}$, is called a \textbf{Markov chain} if and only if,
		\begin{equation*}
			\mathbb{P}[X_{t_{m+1}} = i_{m+1} | X_{t_1}=i_1, \dots, X_{t_m}=i_m] = \mathbb{P}[X_{t_{m+1}} = i_{m+1} | X_{t_m}=i_m]
		\end{equation*}
		for $t_1 < t_2 < \dots < t_m < t_{m+1}$ and $i_1, i_2, \ldots, i_{m+1} \in \mathcal{S}$.
	\end{block}
	We say that such a stochastic process $(X_t)_{t\in \mathbb{N}}$ has no memory.
\end{frame}


\begin{frame}{Markov Chains}
    \begin{block}{Chapman-Kolmogorov Theorem}
		Let $p_{ij}(s,t) = P(X_t = j | X_s = i)$ be the transition probabilities of a Markov chain. Then, for any $0 \leq s < u < t$,
		\begin{equation*}
			p_{ij}(s,t) = \sum_{k} p_{ik}(s,u) p_{kj}(u,t).
		\end{equation*}
		Or written in matrix form, $P(s,t) = P(s,u) P(u,t)$.
	\end{block}
	Idea: What is the probbility of being in state j at time t, given that at time s we are in state i?
\end{frame}

\begin{frame}{Markov Chains}
	\begin{block}{Proof}

		\begin{align*}
			p_{ij}(s,t) &= \mathbb{P}[X_t = j | X_s = i] = \mathbb{P}[X_t = j \cap \bigcup\limits_{k \in \mathcal{S}}\{X_u = k\} | X_s=i] \\
			&= \sum\limits_{k \in \mathcal{S}}\mathbb{P}\left[X_t = j, X_u = k | X_s=i\right] \\
			&= \sum\limits_{k \in \mathcal{S}} \frac{\mathbb{P}[X_t=j, X_u=k, X_s=i]}{\mathbb{P}[X_s=i]} \cdot \frac{\mathbb{P}[X_u=k, X_s=i]}{\mathbb{P}[X_u=k, X_s=i]} \\
			&= \sum\limits_{k \in \mathcal{S}} \underbrace{\mathbb{P}[X_t=j | X_u=k, X_s=i]}_{\mathbb{P}[X_t=j | X_u=k]} \mathbb{P}[X_u=k | X_s=i] \\
			&= \sum\limits_{k \in \mathcal{S}} p_{ik}(s,u) p_{kj}(u,t)
		\end{align*}
		% use Baye's theorem for conditional probabilities: 
	\end{block}
	In the above we used $\mathbb{P} [A \cap B | C] = \mathbb{P}[A | B \cap C] \cdot \mathbb{P}[B | C]$ as well as the Markov porperty 
		as well as assuming that $\mathbb{P}[X_u=k, X_s=i] \neq 0$.
\end{frame}


\begin{frame}{Markov Model}
	To model a life Insurance we need three ingredients:
	\begin{itemize}
		\item a Markov chain $(X_t)_{t in \mathbb{N}}$
		\item a one-year discount factor $v=\tfrac{1}{1+i}$
		\item contract functions $a_{i}^{pre}(t)$ and $a_{ij}^{post}(t)$
	\end{itemize}
	\vskip 0.5cm
	The starting point of an Markov model are the various possible conditions for an insured person, building the state space $\mathcal{S}$.
	E.g. $\mathcal{S} = \{ \text{'living'}, \text{'death'}\}$.
\end{frame}


\begin{frame}{Induced Cashflow \& Mathemtical Reserve}
	A central task in life insurance is the determination of the actuarial reserve, i.e.,
	the amount of money which has to be set aside at a given time t to be able to meet
	all future obligations/benefits towards each policy.\\
	We denote by $A_t$ the payments that are due for a policy at time t.
	$(A_t)_{t \in \mathbb{N}}$ is a stochastic process.
	\begin{equation*}
		A_t = a_{X_t}^{Pre}(t) + a_{X_{t-1} X_{t}}^{Post}(t)
	\end{equation*}
	where $a_{ij}^{Post}(-1)=0$ for all i,j $\in \mathcal{S}$, $t\in \mathbb{N}$.

\end{frame}


\begin{frame}{Induced Cashflow \& Mathemtical Reserve}
	We set $I_i(t) = \mathbbm{1}_{\{X_t = i\}}$. Then we can compute the {\bfseries induced cash flows} as follows:
	\begin{equation*}
		A(t) = \underbrace{\sum\limits_{i \in \mathcal{S}} I_i(t) \cdot a_{i}^{pre}(t)}_{\text{annuity}} + 
		\underbrace{\sum\limits_{i,j \in \mathcal{S}}  I_i(t) \cdot I_j(t+1) \cdot a_{ij}^{post}(t)}_{\text{captial/lump sum paid at time t+1}}
	\end{equation*}
	Idea: $A(t)$ are the payments are due at time t for a given policy. We can also compute the present value (PV) of A(t) which is given by:
	\begin{equation*}
		\tilde{A}(t) = \sum\limits_{i \in \mathcal{S}} I_i(t) \cdot a_{i}^{pre}(t) + v \cdot \sum\limits_{i,j \in \mathcal{S}}  I_i(t) \cdot I_j(t+1) \cdot a_{ij}^{post}(t)
	\end{equation*}
	Finally we can define the mathematical reserve at time t as:
	\begin{align*}
		V_j(t) = \mathbb{E}[\text{PV of future cash flows} | X_t=j] = \mathbb{E}[\sum\limits_{\tau=0}^{\infty} \tilde{A}(t + \tau) | X_t=j]
	\end{align*}
\end{frame}


\begin{frame}{Mathematical Reserve}
	We can compute the mathematical reserves with the following results:
	\begin{align*}
		\mathbb{E}[I_i(t+\tau) | X_t=j] &= p_{ji}(t,t+\tau) \\
		\mathbb{E}[I_i(t+\tau)\,I_k(t+\tau + 1) | X_t=j] &= p_{ji}(t,t+\tau) p_{ik}(t+\tau,t+\tau + 1)
	\end{align*}
	\vskip 0.5cm
	Hence the reserve is given as:
	\begin{align}
		V_j(t) &= \sum\limits_{\tau=0}^{\infty} v^\tau \left(\sum_{i \in \mathcal{S}} a_i^{Pre}(i+ \tau) p_{ji}(t, t+\tau)  \right.\nonumber\\
		&  + \left. v \sum_{i,k \in \mathcal{S}} a_{ik}^{Post}(t+ \tau) p_{ji}(t, t+\tau) p_{ik}(t+\tau,t+\tau + 1) \right)\\
		\label{eq:reserve}\nonumber
	\end{align}
	Hence $V_j(t)$ is the current value of the future actuarial reserve cash flow $(A)$ based on todays information, i.e. $X_t=j$.
\end{frame}


\begin{frame}{Thiele Equation}
	We can relate the mathematical reserves at two subsequent time points t and t+1 via the following equation:
	\begin{block}{Theorem (Thiele's difference equation)}
		The mathematical reserve between two subsequent periods are related by:
		\begin{equation*}
			V_j(t) = a_{j}^{pre}(t) + \sum_{i \in \mathcal{S}} v \cdot p_{ji}(t,t+1) \cdot (a_{ji}^{post}(t) + V_i(t+1))
		\end{equation*}
	\end{block}	
	{\bfseries Remarks:}\\
	\begin{itemize}
		\item Calculates the expected reserve directly using transition probabilities,
		 while simulation estimates it by averaging over many random trajectories
		\item To solve Thiele's equation we need the boundary condition $V_j(T) = 0$ for all $j \in \mathcal{S}$.
		\item The Thiele equation is leads to the same results as for the classical insurance model (using commutation functions)
		\item Forward computation of reserves is possible, too. but numerically less stable.
	\end{itemize}
\end{frame}

\begin{frame}{Proof Thiele Equation}
	% {\bfseries Proof.}
	We start the prove by separating the above sum into $\tau=0$ and the rest:\\
	Lets start with $\tau=0$:
	\begin{align*}
		&\sum_{i \in \mathcal{S}} a_i^{Pre}(t) \underbrace{p_{ji}(t,t)}_{\delta_{ij}} + v \sum_{i,k \in \mathcal{S}} a_{ik}^{Post}(t) p_{ji}(t,t)p_{ik}(t,t+1) \\
		&= a_j^{Pre}(t) + v \sum_{k \in \mathcal{S}} a_{jk}^{Post}(t) p_{jk}(t,t+1)
	\end{align*}
	Continute with $\tau \geq 1$ and using Chapman-Kolmogorov:
	\begin{align*}
		&\sum_{\tau \geq 1} v^\tau \left( \sum_i a_i^{Pre}(t+\tau)\cdot \underbrace{p_{ji}(t,t+\tau)}_{\sum_l p_{jl}(t, t+1)p_{li}(t+1,t+\tau)} \right.\\
		&+ \left. \sum_{i,k} a_{ik}^{Post}(t+\tau) \cdot\underbrace{p_{ji}(t,t+\tau)}_{\sum_l p_{jl}(t, t+1)p_{li}(t+1,t+\tau)} \cdot p_{ik}(t+\tau,t+\tau + 1) \right)
	\end{align*}
\end{frame}

\begin{frame}{Proof Thiele Equation}
	By factor out terms which are independent of $\tau$ as well as rearranging the $\tau$-sum ($\tau-1 \to \tau$) we get:
	\begin{align*}
		&\sum_l p_{jl}(t, t+1) \cdot v \cdot \left( \sum_{\tau \geq 0} v^\tau p_{li}(t+1, t+1+\tau) \right.\\
		&\left.\left[ \sum_i a_i^{Pre}(t+1+\tau) + \sum_{i,k} a_{ik}^{Post}(t+1+\tau)p_{ik}(t+1+\tau, t+\tau+1+1) \cdot v\right] \right)\\
	\end{align*}
	Comparing the expression in $()$ with equation \eqref{eq:reserve} we recognize that this is just $V_l(t+1)$.
	Now combing both parts we get:
	\begin{equation*}
		V_j(t) = a_j^{Pre}(t) + v \sum_{k \in \mathcal{S}} p_{jk}(t,t+1) \left( a_{jk}^{Post}(t) + V_k(t+1) \right)
	\end{equation*}
	This concludes the proof.
\end{frame}


\begin{frame}{How to simulate a trajectory of a discrete time, finite State space Markov Chain}
	
\end{frame}



\begin{frame}{Simulate mathematical reserve}
	
\end{frame}


\begin{frame}{Determination of cumulative Probability Density of the reserves}
	
\end{frame}


\begin{frame}{Simulation of individual cash flows}
	
\end{frame}
% \begin{frame}{Fixed Point Iteration Method}

%   \begin{block}{Definition}<1->
% A point p is a \textbf{fixed point} for a given function g if g(p)= p.
% \end{block}
%  \begin{block}{Remark}<2->
%  Fixed point problems and root finding problems are infact equivalent.
%  \begin{itemize}
% \item<1->if p is a fixed point of the function g, then p is a root of the function
% $$f(x)=[g(x)-x]h(x)$$ [as long as $h(x)\in \mathbb{R}$]
% \item<2->if p is a root of the function of f, then p is a fixed point of the function 
% $$g(x)=x-h(x)f(x)$$ [as long as $h(x)\in \mathbb{R}$]
% \end{itemize}
% \end{block}
%  \end{frame}

\section{Task 2 : Stopping to pay Premium}

\begin{frame}{Mixed Endowment Insurance}
	What is a mixed endowment?
	\begin{itemize}
		\item a mix of a term (temporary death) and a pure endowment insurance
		\item a lump sum is payable on death or reachinga certain age
	\end{itemize}
	Let us look at these random variables individually
	\begin{itemize}
		\item term insurance : $Z_1 = v^{k+1} \mathbbm{1}_{k<n}$ \\
		\vskip 0.2cm
		$A_{x:\overline{n}|}^{\quad 1} = \mathbb{E}[Z_1] = \sum_{k=0}^{n-1} v^{k+1} \prescript{}{k}{p}_x \cdot q_{x+k} = \sum_{k=0}^{n-1} v^{k+1} \cdot\tfrac{l_{x+k}}{l_x}\cdot\tfrac{d_{x+k}}{l_{x+k}} $ \\ \vskip 0.2cm $= \sum_{k=0}^{n-1} \frac{C_{x+k}}{D_x}= \frac{M_x - M_{x+n}}{D_x}$
		% 						&= \sum_{k=0}^{n-1} \frac{C_{x+k}}{D_x} = \frac{M_x - M_{x+n}}{D_x}
		% \begin{align*}
		% 	A_{x:\overline{n}|} = \mathbb{E}[Z_1] &= \sum_{k=0}^{n-1} v^{k+1} \prescript{}{k}{p}_x \cdot q_{x+k} = \sum_{k=0}^{n-1} v^{k+1} \cdot\tfrac{l_{x+k}}{l_x}\cdot\tfrac{d_{x+k}}{l_{x+k}} \\
		% 						&= \sum_{k=0}^{n-1} \frac{C_{x+k}}{D_x} = \frac{M_x - M_{x+n}}{D_x}
		% \end{align*}
		\item pure endowment: $Z_2 = v^{k+1} \mathbbm{1}_{k\geq n}$
		\item $ A_{x:\overline{n}|}^{1} = \mathbb{E}[Z_2] = \sum_{k=n}^{\infty} v^n \mathbb{P}[K=k] = v^n \mathbb{P}[K \geq n] = v^n \cdot \prescript{}{n}{p}_x = \frac{D_{x+n}}{D_x}$
		
		\item Thus the epxectation value for the mixed enovment is given by \\
			$$ A_{x:\overline{n}|} = \mathbb{E}[Z] = \mathbb{E}[Z_1] + \mathbb{E}[Z_2] = \frac{M_x - M_{x+n} + D_{x+n}}{D_x}$$
	\end{itemize}
	% \begin{itemize}
	% 	\item Consider a mixed endowment $A_{80:\overline{10}|}$ maturing at age 90 with benefit insured $L=100'000$.
	% 	\item Techinical interest rate $i=2\%$ and Preimum is paid anually prenumerando
	% \end{itemize}
\end{frame}

\begin{frame}{Equivalence Principle}
	In order to determine the premium one typically uses the Equivalence Principle
	\begin{enumerate}
		\item $\mathbb{E}[L] = 0$ where $L$ denotes the loss, or equivalently,
		\item The expected value of premiums is equal to the expected value of benefits.
	\end{enumerate}
	Remark: \\
	\begin{itemize}
		\item The Equivalece Principle is equivalent to the requirement of the mathematical reserve at inception to be zero.
	\end{itemize}
\end{frame}


\begin{frame}{Mathemical Reserves}
	Mathematical Reserves at given time = PV(future benefits) - PV(future premiums)\\
	\vskip 0.5cm
	Expressed in commutation functions:
	\begin{equation*}
		\prescript{}{t}{V}_x = \frac{M_{x+t} - M_{x+n} + D_{x+n} - \Pi\cdot (N_{x+t} - N_{x+n})}{D_{x+t}}
	\end{equation*}
\end{frame}


\begin{frame}{2.1 Premium for Product at Inception}
	\begin{itemize}
		\item Compute the premium by means of the equivalence principle.
		
		\item For the mixes endowment with benefit $L$ we have
				\begin{equation*}
					\Pi = L \frac{A_{x:\overline{n}|}}{\ddot{a}_{x:\overline{n}|}} = L \left(\frac{M_x - M_{x+n} + D_{x+n}}{N_x - N_{x+n}}\right)
				\end{equation*}
	\item Plugging in the values we get $\Pi \approx 12'302.98$
	\end{itemize}
\end{frame}


\begin{frame}{2.2 Benefit $\tilde{L}$ after policyholder stops after one premium. }
	\begin{itemize}
		\item Combining the formulas for the mixed endowment insurance $A_{x+k : \overline{n-k}|}$ and the mathematical reserve $\prescript{}{k}{V}_x$
			we obtain the following expression for the reduced benefit using commutation functions
			\begin{equation*}
					\prescript{}{1}{\tilde{L}} = \frac{\prescript{}{k}{V}_{x}}{A_{x+k:\overline{n-k}|}} = \prescript{}{k}{V}_x \cdot \frac{D_{x+k}}{M_{x+k} - M_{x+n} + D_{x+n}}
			\end{equation*}
		\item Result of the python code is: $\prescript{}{1}{\tilde{L}} \approx 9228.77$.
	\end{itemize}
	
\end{frame}


\begin{frame}{2.3 Benefit Level $\tilde{L}$ as a function of the number of paid premiums}
	\begin{itemize}
		\item Let $k$ be the number of paid premiums then by the previous exercise
			\begin{equation*}
				\prescript{}{1}{\tilde{L}} = \frac{\prescript{}{k}{V}_{x}}{A_{x+k:\overline{n-k}|}} = \prescript{}{k}{V}_x \cdot \frac{D_{x+k}}{M_{x+k} - M_{x+n} + D_{x+n}}
			\end{equation*}
		\begin{table}[h]
  			\begin{tabular}{|c|c|c|}
    		\hline
    		$k$ & Reserve  $\prescript{}{j}{V}_x$ &  $\text{Benefit Level }\prescript{}{k}{\tilde{L}}$ \\
    		\hline
    		1 & 8062.41 & 9228.77 \\
    		2 & 16260.21 & 18375.48 \\
			3 & 24650.21 & 27498.51 \\
			4 & 33308.23 & 36670.15 \\
			5 & 42335.99 & 45980.83\\
			6 & 51870.01 & 55545.00 \\
			7 & 62095.67 & 65509.06 \\
			8 & 73266.94 & 76062.14 \\
			9 & 85736.24 & 87450.96 \\
			10 & 100000.0 & 100000.0\\
			\hline
  			\end{tabular}
			\end{table}
	\end{itemize}	
\end{frame}


\begin{frame}{2.4 Which equivalence principle is fulfilled for the first premium assuming only one premium is paid.}
	\begin{itemize}
		\item $t=0$ and premium $P$ 
		\item Payout/Benefits:\\ i.) if $t \in [0,1)$ then $L=100'000$ ii.) if $t \in [1,10)$ then $\prescript{}{t}{\tilde{L}}$
		\item by the {\bfseries Equivalence Principle}: $\mathbb{E}(premium) = \mathbb{E}(benefits)$
		\item the benefits are computed as
			\begin{align*} 
				\mathbb{E}(benefits) &= v \cdot q_x \cdot L + v^2 \cdot \prescript{}{1}{\tilde{L}} \cdot p_x \cdot q_{x+1} + \dots + v^{10} \cdot \prescript{}{1}{\tilde{L}} \cdot p_{x+10}\\
												&= v \cdot q_x \cdot L + v \cdot p_x \cdot \prescript{}{1}{\tilde{L}}\cdot A_{x+1 : \overline{9}|}
			\end{align*}
		\item A short check in the jupyter notebook shows that $\mathbb{E}(premium) = \mathbb{E}(benefits)$
	\end{itemize}
\end{frame}


\begin{frame}{2.5 Which equivalence principle is fulfilled for the second premium assuming only one premium is paid.}
	\begin{itemize}
		\item t=1
		\item Assuming one premium is paid and compute the second one
		\item Assuming that insured person survives the first year
		\item Insurer is at risk to pay $L$ if the death occurs in $[1,2)$
		\item From time $t=2$ on until maturity the insruer is at risk of $\prescript{}{2}{\tilde{L}}_x$
		\item By the Equivalence Principle $\mathbb{E}(premium) = \mathbb{E}(benefits)$
		\item $\mathbb{E}(premium) = P + \prescript{}{1}{V}_x$
	\end{itemize}
\end{frame}

\begin{frame}{2.5 Which equivalence principle is fulfilled for the second premium assuming only one premium is paid.}
	\begin{itemize}
		\item 	Similar to before we compute the benefits:
				\begin{align*}
					\mathbb{E}(benefits) = v \cdot q_{x+1} \cdot L + v \cdot p_{x+1} \cdot \prescript{}{2}{\tilde{L}} \cdot A_{x+2 : \overline{8}|}
				\end{align*}

		\item A short check in the jupyter notebook shows that $\mathbb{E}(premium) = \mathbb{E}(benefits).$
	\end{itemize}
	
\end{frame}





\section{Task 3: Disability Insurance on two lives}

\begin{frame}{Problem}
	
\end{frame}

\begin{frame}{3.1 }
	Two random variables $X$ and $Y$ are \textbf{stochastically independent} if and only if their joint probability distribution factorizes:
	\begin{equation*}
		P(X = x, Y = y) = P(X = x) \cdot P(Y = y) \quad \forall x, y
	\end{equation*}
	Or equivalently:
	\begin{equation*}
		P(Y = y \mid X = x) = P(Y = y) \quad \forall x, y.
	\end{equation*}
	\textit{Intuitively:} Knowing $X$ provides no information about $Y$\\
	\vskip 0.3cm
	Two random variables $X$ and $Y$ are \textbf{uncorrelated} if their covariance is zero: $$
	\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = 0.$$
	Or equivalently: 
	\begin{equation*}
	\quad \mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y].
	\end{equation*}
	% \textit{In other words} there is no \textbf{linear} relationship between $X$ and $Y$.

	\end{frame}

	\begin{frame}{3.1}
		\begin{block}{Independence $\Rightarrow$ Uncorrelated}
		If $X$ and $Y$ are independent, then they are uncorrelated.

	\textbf{Proof:}
	\[
	\mathbb{E}[XY] = \sum_x \sum_y xy \cdot P(X=x) \cdot P(Y=y) = \mathbb{E}[X] \cdot \mathbb{E}[Y]
	\]
	\end{block}

	\vspace{0.5cm}

	\begin{alertblock}{Uncorrelated $\not\Rightarrow$ Independence}
	Two variables can be uncorrelated yet still be dependent!
	\end{alertblock}
\end{frame}


\begin{frame}{3.1 Counterexample: Uncorrelated but Dependent}
% \frametitle{Uncorrelated but Dependent}

Let $X \sim \text{Uniform}(-1, 1)$ and $Y = X^2$

\vspace{0.3cm}

\begin{itemize}
    \item $\mathbb{E}[XY] = \mathbb{E}[X^3] = 0$ (by symmetry)
    \vspace{0.2cm}
    \item $\mathbb{E}[X] = 0$ and $\mathbb{E}[Y] = \frac{1}{3}$
    \vspace{0.2cm}
    \item Therefore: $\mathbb{E}[X] \cdot \mathbb{E}[Y] = 0$
    \vspace{0.2cm}
    \item So $\text{Cov}(X,Y) = 0$ $\Rightarrow$ \textbf{uncorrelated}
    \vspace{0.2cm}
    \item But $Y$ is completely determined by $X$ $\Rightarrow$ \textbf{dependent!}
\end{itemize}

\vspace{0.5cm}

% \textbf{Key insight:} Independence means \textit{no relationship whatsoever}, while being uncorrelated only rules out \textit{linear} relationships.

\end{frame}


\begin{frame}{3.2}
	
\end{frame}

% \section{Appendix}





%  \begin{frame}{Fixed Point Iteration Method}

%  \begin{block}{Definition}<1->
% Let U be a subset of a metric space X.
% \\A function g:U $\rightarrow$X called \textbf{Lipschitz continuous} provided there exists a constant $\lambda\ge$ 0 (called Lipschitz constant)
% \\such that $\forall$ x,y$\in$U d(g(x),g(y))$\le$d(x,y)
% \\if $\lambda\in$[0,1], then g is called \textbf{contraction} (with contraction constant $\lambda$).
% \end{block} 
%  \begin{block}{Theorem (A Fixed Point Theorem)}<2->
%  Suppose $g:[a,b]\rightarrow[a,b]$ is continuous. Then g has a fixed point.
 

% \end{block}
% \end{frame}
%  \begin{frame}{Fixed Point Iteration Method}
% \begin{block}{Lemma}<1->
%  A contraction has at most one fixed point.
 

% \end{block}
% \begin{block}{Corollary}<2->
% Suppose $g:[a,b]\rightarrow[a,b]$ is continuous and $\lambda:=sup\mid g'(x)\mid<1$ for $x\in (a,b)$
% \\Then g is a contraction with contraction constant $\lambda.$
% \end{block}
% \vskip 1cm

% \end{frame}

% \begin{frame}{Graphical determination of the existence of a fixed point for the function $g(x)= \frac{x^2-3}{2}$}

% \includegraphics[width=355px, height= 200px]{Figure2.jpg}
% \end{frame}



% \subsection{Banach Fixed Point Theorem}

% \begin{frame}{Banach Fixed Point Theorem}

% \begin{theorem}[Banach Fixed Point Theorem]
%   \begin{itemize}


%  \item[]<1-> Let U be a complete subset of a metric space X, and let g:U$\rightarrow$U be a contraction with contraction constant $\lambda$.
%   \\Then  \begin{itemize}
%     \item<2-> g has a unique fixed point, say p.
%     \item<2->For any sequence $\left\{x_{n}\right\}$ defined by $x_n$=g($x_{n-1}$), n=1,2,,...
%     \\converges to this unique fixed point p.
%     \item[]<3->Moreover, we have the  \textbf{a priori} error estimate
%     $$ \mid x_n-p\mid\le\frac{\lambda^n}{1-\lambda}\mid x_1-x_0\mid$$
%     \item[]<4->and the \textbf{a posteriori} error estimate 
%       $$\mid x_n-p\mid\le\frac{\lambda}{1-\lambda}\mid x_n-x_{n-1}\mid$$
% \end{itemize}
% \end{itemize}  
% \end{theorem}
% \vskip 1cm


% \end{frame}


% \begin{frame}{Banach Fixed Point Theorem}

% \begin{block}{Proof}

% For $n > m$ , we have
% \begin{align*}
% \mid x_n-x_m\mid&=\mid x_n-x_{n-1}+x_{n-1}-x_{n-2}+...+x_{m+1}-x_{m}\mid\\
% &\le\mid x_n-x_{n-1}\mid + \mid x_{n-1}-x_{n-2}\mid+...+\mid x_{m+1}-x_{m}\mid\\
% &by  *\\
% &\le(\lambda^{n-1}+\lambda^{n-2}+...+\lambda^{m})\mid x_1-x_0\mid\\
% &=\lambda^{m}(\lambda^{n-m-1}+\lambda^{n-m-2}+...+1)\mid x_1-x_0\mid\\
% &=\lambda^{m}\frac{1-\lambda^{n-m}}{1-\lambda}\mid x_1-x_0\mid\le \frac{\lambda^m}{1-\lambda}\mid x_1-x_0\mid
%  \end{align*}
 
%  so that ${x_n}$ is Cauchy sequence in U.
%  \\Since U is complete, ${x_n}$ converges to a point p $\in$ U
% $$*\color{red}\mid x_k-x_{k-1}\mid=\mid g(x_{k-1})-g(x_{k-2})\mid\le\lambda\mid x_{k-1}-x_{k-2}\mid\le..\le\lambda^{k-1}\mid x_1-x_0\mid\color{black}$$

% \end{block}

% \vskip 1cm

% \end{frame}


% \begin{frame}
% \begin{proof}[Continue]
% \begin{itemize}

% \item[]<1->Now,since g being contraction is continuous, we have 
% $$p=\lim_{n\to\infty}x_{n}=\lim_{n\to\infty}g(x_{n-1})=g(lim_{n\to\infty}x_{n-1})=g(p)$$
%  \\so that p is fixed point of g.
%  \\By the lemma p is the unique fixed point of g.
%  \item[]<2->Since $$\mid x_n-x_m\mid\le \frac{\lambda^m}{1-\lambda}\mid x_1-x_0\mid,$$
%  \\letting n$\rightarrow\infty$
%  \item[]<3->we get $$\mid p-x_m\mid\le \frac{\lambda^m}{1-\lambda}\mid x_1-x_0\mid$$ 
%  \\for $y_0=x_{n-1},$ $y_1= x_n$
% $$\mid y_1- p\mid \le \frac{\lambda}{1-\lambda}\mid y_1-y_0\mid$$
%  \end{itemize}
%  \end{proof}
% \vskip 1cm

% \end{frame}

% \subsection{The Fixed Point Algorithm}

% \begin{frame}{The Fixed Point Algorithm}



% \begin{block}{The Fixed Point Algorithm}<1->


% If g has a fixed point p, then the fixed point algorithm generates a sequence $\left\{x_n\right\} $ defined as
% \\$x_0$: arbitrary but fixed,
% \\$x_n=g(x_{n-1})$, n=1,2,3,... to approximate p.
% \end{block}

% \end{frame}

% \subsection{Fixed Point The Case Where Multiple Derivatives Are Zero at The Fixed Point}
% \begin{frame}{Fixed Point The Case Where Multiple Derivatives Are Zero at The Fixed Point}
% \begin{block}{Theorem}
% \begin{itemize}

% \item[]<1->Let g be a continuous function on the closed internal $[a,b]$ with $\alpha>1$ continuous derivatives on the internal (a,b). 
% \\Further, Let p $\in $(a,b) be a fixed point of g. 
% \item[]<2->if $$g'(p)=g''(p)=...=g^{(\alpha-1)}(p)=0$$ but $g^{(\alpha)}(p)\neq 0$, 

% \item[]<3->then there exist a $\delta>0$ such that for any $p_0\in[p-\delta,p+\delta]$, the sequence $p_n=g(p_{n-1})$ converges to the fixed point p of order $\alpha$ with asymtotic error constant $$\lim_{n\to\infty}\frac{\mid e_{n+1}\mid}{\mid e_n\mid^\alpha}=\frac{\mid g^{(\alpha)}(p)\mid}{\alpha!}$$.

% \end{itemize}
% \end{block}
% \end{frame}


% \begin{frame}

% \begin{block}{Proof}
% \begin{itemize}


% \item[]<1->Let$'$s start by establishing the existence of $\delta>0$ such that for any $p_0\in [p-\delta,p+\delta]$, the sequence $p_n=g(p_{n-1})$ converges to the fixed point p.

% \item[]<2->Let $\lambda<1$. Since $g'(p)=0$ and $g'$ is continuous, it follows that there exists a $\delta>0$ such that$\mid g'(x)\mid\le \lambda<1$ for all $x\in I \equiv[p-\delta,p+\delta]$ From this, it follows that g:I$\rightarrow$I; for if x$\in$ I then,
% \item[]<3->
% \begin{align*}
% \mid g(x)-p\mid&=\mid g(x)-g(p)\mid\\
% &=\mid g'(\xi)\mid\mid x-p\mid\\
% &\le \lambda\mid x-p\mid<\mid x-p\mid\\
% &\le\delta
% \end{align*}
% \item[]<4->Therefore by the a fixed point theorem established earlier, the sequence $p_n=g(p_{n-1})$ converges to the fixed point p for any $p_0\in [p-\delta,p+\delta]$.

% \end{itemize}
% \end{block}

% \vskip 1cm

% \end{frame}



% \begin{frame}

% \begin{block}{Continue}
% \begin{itemize}


% \item[]<1->To establish the order of convergence, let x$\in$ I and expand the iteration function g into a Taylor series about x=p:
% $$g(x)=g(p)+g'(p)(x-p)+...+\frac{g^{\alpha-1}(p)}{(\alpha-1)!}(x-p)^{\alpha-1}+\frac{g^{\alpha}(\xi)}{(\alpha)!}(x-p)^{\alpha}$$ 
% \\where $\xi$ is between x and p. 
% \item[]<2->Using the hypotheses regarding the value of $g^{(k)}(p)$ for $1\le k\le \alpha-1$ and letting $x=p_n$, the Taylor series expansion simplifies to $$p_n+1-p=\frac{g^(\alpha)(\xi)}{\alpha!}(p_n-p)^\alpha$$
% \\where $\xi$ is now between $p_n$ and p.

% \end{itemize}
% \end{block}

% \vskip 1cm

% \end{frame}


% \begin{frame}

% \begin{proof}[Continue]
% \begin{itemize}
% \item[]<1->The definitions of fixed point iteration scheme and of a fixed point have been used to replace $g(p_n)$ with $p_{n+1}$ and g(p) with p.

%  \item[]<2->Finally, let $n\rightarrow\infty$.Then $p_n\rightarrow p$, forcing $\xi\rightarrow$ p also.
% Hence $$\lim_{n\to\infty}\frac{\mid e_{n+1}\mid}{\mid e_n\mid^\alpha}=\frac{\mid g^{(\alpha)}(p)\mid}{\alpha!}$$
% \\or $p_n\rightarrow$ p of order $\alpha$.

% \end{itemize}
% \end{proof}


% \end{frame}
% \section{Steffensen's Acceleration Method}

% \subsection{Aitken's $\Delta^2$ Method}
% \begin{frame}

% \begin{theorem}[Aitken's $\Delta^2$ method]
% \begin{itemize}
% \item[] <1-> Suppose that
% \begin{itemize}
% 	\item $\left\{x_{n}\right\}$ is a sequence with $x_{n}\neq$ p for all n $\in \mathbb{N}$ 
% 	\item there is a constant c $\in \Re\setminus \left\{ \mp 1 \right\}$ and a sequece $\left\{ \delta_{n} \right\}$

% 		such that 
% 		\begin{itemize}
% 			\item $\lim_{n\rightarrow\infty}\delta_{n}=0$
% 			\item $x_{n+1}-p=(c+\delta_{n})(x_{n}-p)$ for all n $\in  \mathbb{N}$
% 		\end{itemize}
% 	\end{itemize}
% 	\item[] <2->Then
	
% 	\begin{itemize}
% 		\item $\left\{x_{n}\right\}$ converges to p iff $\left|c\right|<1$
% 		\item if $\left|c\right|<1$ , then 
% 	\[ 
% 	y_{n}=\frac{x_{n+2}x_{n}-x_{n+1}^2}{x_{n+2} - 2x_{n+1}+x_{n}}= x_n-\frac{(x_{n+1}-x_n)^2}{x_{n+2}-2x_{n+1}+x_n}
% 	\] 
% 		\end{itemize}
% 		\      is well-defined for all sufficiently large n.
% 		\item[]<3-> Moreover $\left\{y_{n}\right\}$ converges to p faster than $\left\{x_{n}\right\}$ in the sense that
% 		\[ 
% 		\lim_{n\rightarrow\infty} \frac{y_{n}-p}{x_{n}-p}=0
% 		\]
%  \end{itemize}

% \end{theorem}

% \vskip 1cm

% \end{frame}

% \begin{frame}
% \begin{proof}
% \begin{itemize}
% 	\item[]<1-> Let $e_{n} = x_{n}-p$ 
% 	\item[]<2-> $y_{n}-p=\frac{x_{n+2}x_{n}-x_{n+1}^2}{x_{n+2} - 2x_{n+1}+x_{n}}-p$
% 	\item[]<3-> $y_{n}-p$=$\frac{(e_{n+2}+p)(e_{n}+p)-(e_{n+1}+p)^2}{(e_{n+2}+p)-2(e_{n+1}+p)+(e_{n}+p)}$
% 	\item[]<4-> $y_{n}-p$=$\frac{e_{n+2}e_{n}-e_{n}^2}{e_{n+2}-2e_{n+1}+e_{n}}$
% 	\item[]<5-> since we have
% 	\item[]<5-> $x_{n+1}-p= (c+\delta_{n})(x_{n}-p)$ 
% 	\ and $e_{n+1}=(c+\delta_{n}e_{n})$
% 	\item[]<6-> $y_{n}-p$=$\frac{(c+\delta_{n+1})(c+\delta_{n})e_{n}e_{n}-(c+\delta_{n})^2e_{n}^2}{(c+\delta_{n+1})(c+\delta_{n})e_{n}-2(c+\delta_{n})e_{n}+e_{n}}$
% 	\item[]<7-> $y_{n}-p$=$\frac{(c+\delta_{n+1})(c+\delta_{n})-(c+\delta_{n})^2}{(c+\delta_{n+1})(c+\delta_{n})-2(c+\delta_{n})+1}$ $(x_{n}-p)$
% 	\item[]<8-> $y_{n}-p$=$\frac{(c+\delta_{n})(\delta_{n+1}-\delta_{n})}{(c-1)^+c(\delta_{n+1}+\delta_{n})+\delta_{n}(\delta_{n+1}-2)}$
% 	\item[]<9-> Therefore $\lim_{n\rightarrow\infty} \frac{y_{n}-p}{x_{n}-p}=0$
% \end{itemize}
% \end{proof}

% \end{frame}
% \subsection{Steffensen's Acceleration Method}
% \begin{frame}
% \begin{block}{Steffensen's Acceleration Method}
% \begin{itemize}



% \item[]<1->Steffensen’s Method is a combination of fixed-point iteration and the Aitken’s $\Delta^2$ method: 

% 	\item[]<2->Suppose we have a fixed point iteration: $$x_0,x_1=g(x_0),x_2=g(x_1),...$$
% 	\item[]<3->Once we have $x_0, x_1,$ and $x_2,$ we can compute $$ y_0=x_0-\frac{(x_1-x_0)^2}{x_2-2x_1+x_0}$$ 
% At this point we "restart" the fixed point iteration with $x_0=y_0$
% 	\item[]<4->e.g.\color{brown} $$x_3=y_0, x_4=g(x_3), x_5=g(x_4),$$\color{black}
% and compute \color{brown} $$y_3=x_3-\frac{(x_4-x_3)^2}{x_5-2x_4+x_3}$$ \color{black}

% \end{itemize}

% \end{block}
% \end{frame}
% \section{Comparison With Fixed point iteration and Steffensen's Acceleration Method}
% \begin{frame}{Comparison with Fixed Point Iteration and Steffensen's Acceleration Method}
% \begin{block}{EXAMPLE}
% \begin{itemize}


% \item[]<1-> Use the Fixed Point iteration method to find a solution to $f(x) = x^2-2x-3$ using $x_0=0$, tolerance =$10^{-1}$ and compare the approximations with those given by Steffensen's Acceleration method with $x_0=0$, tolerance = $10^{-2}$.
% \item<2-> We can see that my MATLAB code while  Fixed Point iteration method reaches the root by 788 iteration, Steffensen's Acceleration method reaches the root by only 3 iterations. 
% \end{itemize}

% \end{block}

% \end{frame}



\end{document}